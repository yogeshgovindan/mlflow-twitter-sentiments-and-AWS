{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\acer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2024/09/19 00:29:13 INFO mlflow.tracking.fluent: Experiment with name 'NLP_Twitter_experiment' does not exist. Creating a new experiment.\n",
      "2024/09/19 00:29:38 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'TwitterSentimentModel' already exists. Creating a new version of this model...\n",
      "2024/09/19 00:29:49 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: TwitterSentimentModel, version 2\n",
      "Created version '2' of model 'TwitterSentimentModel'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logged and registered successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/19 00:29:50 INFO mlflow.tracking._tracking_service.client: üèÉ View run Logistic Regression at: http://ec2-18-234-243-105.compute-1.amazonaws.com:5000/#/experiments/389111837301992168/runs/359671704bb1493c826973cd7ea3ae33.\n",
      "2024/09/19 00:29:50 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://ec2-18-234-243-105.compute-1.amazonaws.com:5000/#/experiments/389111837301992168.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to evaluate metrics\n",
    "def eval_metrics(actual, pred):\n",
    "    accuracy = accuracy_score(actual, pred)\n",
    "    precision = precision_score(actual, pred, average='weighted')\n",
    "    recall = recall_score(actual, pred, average='weighted')\n",
    "    f1 = f1_score(actual, pred, average='weighted')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Function for text preprocessing\n",
    "\n",
    "# Download NLTK stopwords and punkt if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the text by normalizing, removing special characters, punctuation,\n",
    "    stop words, and HTML tags.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The input text to preprocess.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The preprocessed text.\n",
    "    \"\"\"\n",
    "    # 1. Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    # 3. Remove URLs\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "\n",
    "    # 4. Remove special characters and punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # 5. Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # 6. Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 7. Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # 8. Rejoin tokens into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     # Add your text preprocessing steps here (normalization, removing special characters, etc.)\n",
    "#     return text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    np.random.seed(40)\n",
    "\n",
    "     # Set the experiment name\n",
    "    mlflow.set_experiment(\"NLP_Twitter_experiment\")\n",
    "\n",
    "    # Load the Twitter sentiment dataset\n",
    "    data = pd.read_csv(\"twitter_dataset.csv\")  # Update with your dataset path\n",
    "    data['text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "    # Encode the labels (assuming they are in 'airline_sentiment' column)\n",
    "    data['label'] = data['airline_sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
    "\n",
    "    # Split the data into training and test sets (0.75, 0.25)\n",
    "    train, test = train_test_split(data, test_size=0.25, random_state=42)\n",
    "\n",
    "    train_x = train['text']  # Features\n",
    "    train_y = train['label']  # Labels\n",
    "    test_x = test['text']\n",
    "    test_y = test['label']\n",
    "\n",
    "    # Convert text to numerical representation (e.g., using TfidfVectorizer)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    train_x_vectorized = vectorizer.fit_transform(train_x)\n",
    "    test_x_vectorized = vectorizer.transform(test_x)\n",
    "\n",
    "    # Set the tracking URI for MLflow (remote server)\n",
    "    remote_server_uri = \"http://ec2-18-234-243-105.compute-1.amazonaws.com:5000/\"\n",
    "    mlflow.set_tracking_uri(remote_server_uri)\n",
    "\n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=\"Logistic Regression\"):\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        model.fit(train_x_vectorized, train_y)\n",
    "\n",
    "        # Predictions\n",
    "        predicted_labels = model.predict(test_x_vectorized)\n",
    "        accuracy, precision, recall, f1 = eval_metrics(test_y, predicted_labels)\n",
    "\n",
    "        # Log parameters and metrics\n",
    "        mlflow.log_param(\"model_type\", \"Logistic Regression\")\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "        # Log the model\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        # Model registry does not work with file store\n",
    "        if tracking_url_type_store != \"file\":\n",
    "            mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"TwitterSentimentModel\")\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "        print(\"Model logged and registered successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ygdemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
